
# Redis — расширенная шпаргалка (единый документ, темы‑блоки)

> Формат: лаконичные пункты, но с пояснениями «что это, зачем и как».  
> Таблицы — только там, где они действительно помогают «схватить» суть.

---

## 1. Что это (чем Redis отличается от «обычной» БД)

| Факт | Расшифровка |
|------|-------------|
| **In‑memory data‑store** | Все рабочие данные живут в RAM ⇒  миллисекундный RTT (Round‑Trip‑Time — время от запроса до ответа). |
| **Кеш, БД и брокер** | Можно держать: кеш API, сессии, счётчики, очереди задач, Pub/Sub‑шину. |
| **Schema‑free** | Ключ и значение — любые байты; _«сериализацию выбираешь сам»_: хочешь — кладёшь JSON, ProtoBuf, простой текст, хеш‑таблицу… |
| **Однопоточный I/O‑loop** | Один рабочий поток обрабатывает команды последовательно → нет блокировок, а latency предсказуем. |
| **Лёгкая конфигурация** | Минимальный `redis.conf` запускается «из коробки»: 1 порт, 1 файл конфига, 1 exec‑файл — и готово. |
| **Персистенция (не обязательная)** | RDB‑снимки +/‑ AOF‑журнал — можно включить оба, один или выключить совсем. |

---

## 2. Структуры данных — что хранить и какими командами

| Тип             | Сценарий   (пример запроса бизнеса)                                                                                | Почему именно он «в темe»                                                                                                       | Ключевые команды                      |
| --------------- | ------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------- |
| **String**      | • Счётчик лайков поста  <br>• JWT‑токен для 15 мин сессии                                                          | • Простой скаляр, занимает минимум памяти  <br>• `INCR/DECR` — атомарны → безопасно считать                                     | `SET/GET`, `INCRBY`, `APPEND`         |
| **Hash**        | • Профиль пользователя `user:42` (`name`, `level`, `coins`)  <br>• Конфиг фич‑флага (`enabled:true`, `rollout:30`) | • Группируем логически связанные поля под одним ключом  <br>• Читаем/пишем отдельное поле без таскания всей структуры           | `HSET`, `HGET`, `HGETALL`             |
| **List**        | • Очередь email‑задач FIFO  <br>• Лог последних 100 событий для отладки                                            | • Константный `LPUSH`/`RPOP` → быстрая очередь  <br>• Можно обрезать хвост `LTRIM` для «скользящего окна»                       | `LPUSH`, `BRPOP`, `LTRIM`             |
| **Set**         | • Множество активных IP‑адресов за день  <br>• Теги статьи (`tags:42`)                                             | • Гарантирует уникальность, моментальный `O(1)` поиск `SISMEMBER`  <br>• Поддерживает математику множеств (`SUNION`, `SINTER`)  | `SADD`, `SISMEMBER`, `SUNION`         |
| **Sorted Set**  | • Leader‑board игры по очкам  <br>• Priority Queue: задачи со Score = время дедлайна                               | • Каждый элемент хранится с числовым `score` → отсортирован «сам по себе»  <br>• Получить топ‑N `ZRANGE 0 N` без доп‑сортировок | `ZADD`, `ZRANGE/ZREVRANGE`, `ZPOPMIN` |
| **Bitmap**      | • Отметить, заходил ли юзер ID за день (`SETBIT uid 1`)                                                            | • 1 бит на пользователя → десятки млн записей в сотнях килобайт  <br>• `BITCOUNT` мгновенно считает DAU                         | `SETBIT`, `GETBIT`, `BITCOUNT`        |
| **HyperLogLog** | • Приблизительное число уникальных посетителей сайта за месяц                                                      | • Фиксированный объём ≈ 12 КБ на ключ, даже для миллиардов элементов  <br>• Погрешность ~1 % приемлема для аналитики            | `PFADD`, `PFCOUNT`, `PFMERGE`         |
| **Stream**      | • Лента заказов e‑commerce (Producer → Consumer Group)  <br>• Log click‑stream с ack‑механикой                     | • Append‑only + авто‑ID ⇒ порядок событий гарантирован  <br>• Consumer Groups позволяют горизонтально обрабатывать очередь      | `XADD`, `XREADGROUP`, `XACK`, `XTRIM` |

---

## 3. Как работает персистенция

| Механизм | Как записывает | Что даёт | Подводные камни |
|----------|---------------|----------|-----------------|
| **RDB (snapshot)** | Фоновый `fork()` → один файл `dump.rdb` через заданные интервалы (`save 900 1` …) | Компактный бэкап, быстро грузится | Во время `fork` чтение/запись «подмораживается»; всё, что между снапшотами, можно потерять |
| **AOF (append‑only‑file)** | Каждая операция записи → лог‑файл. flush диску по режиму `appendfsync` | Восстанавливает _пошагово_, потеря ≤ настройки flush | Файл растёт; при рестарте нужно «прокрутить» лог (rewrite решает, но жрёт I/O) |

### `appendfsync` — что значат режимы

| Режим | Когда fsync | Потеря данных при крэше | Цена |
|-------|-------------|------------------------|------|
| `always` | После **каждой** записи | ≈ 0 байт | + 50‑80 % latency, дисковый I/O — огонь |
| `everysec` **(дефолт)** | Раз в 1 секунду | ≤ 1 секунда операций | Хороший баланс |
| `no` | ОС сама решит | Секунды‑десятки секунд | Максимум TPS, минимум гарантии |

> ВАЖНО!
>  Всё сказанное выше применимо, если Master Redis живёт без реплик, используется как основное хранилище и допустимо ждать часы его возвращения.

---

## 4. Ограничение памяти и выбор eviction‑политики

| Параметр | Что делает | Когда выбирать |
|----------|-----------|----------------|
| `maxmemory` | Жёсткий лимит RAM, по достижении запускается eviction | Ставь ≤ 80 % физической RAM |
| **Политики** (`maxmemory-policy`) | | |
| `noeviction` | Ошибка `OOM command not allowed` | Только если критически важно не терять данные |
| `volatile-lru / volatile-lfu` | LRU/LFU, **только** ключи с TTL | Классический кеш |
| `allkeys-lru / allkeys-lfu` | LRU/LFU для **всех** ключей | «Пусть Redis сам решает» |
| `volatile-ttl` | Самый скорый истекает первым | TTL‑heavy данные |

---

## 5. Масштабирование и HA

### 5.1 Репликация

* **Master → Replica(s)** (асинхронно).  
  - Чтения можно увести на реплики.  
  - При падении мастера Sentinel/Cluster делает **promote** реплики.

### 5.2 Шардирование — Redis Cluster

* **16384 hash‑slot’ов** → распределены по N узлам‑мастерам.  
* На каждом мастере ≥ 1 реплика.  
* Клиент библиотекой знает «карту слотов» (redirect `MOVED`).  
* Добавление/удаление узлов — _reshard_, слоты перетекают без даунтайма.

| Конфигурация | Что это | Плюсы | Минусы |
|--------------|--------|-------|--------|
| **Single Master + 1‑2 Replicas** | Простой HA без шардирования, Sentinel | Лёгко внедрить | RAM мастера — потолок |
| **Cluster 3 Masters × 2 Replicas** | 3 шарда, 2 копии каждого | Горизонтальный рост, auto‑failover | Клиент должен уметь Cluster |
| **Geo‑Cluster (Active‑Active)** | Несколько кластеров, асинхроничная репликация | DR‑схема | Конфликты данных; latency |

### 5.3 Работа hash-слотов в Redis

| Пункт                            | Подробное объяснение                                                                                                                                                           |
| -------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **1. Что такое hash slot**       | В Redis Cluster всё пространство ключей разбито на **16 384 «корзинки»** (слоты с номерами `0…16383`).                                                                         |
| **2. Как ключ попадает в слот**  | На клиенте считается `CRC16(key) % 16384`. Например: `CRC16("user:42") = 15092` ⇒ этот ключ навсегда относится к слоту **15092**.                                              |
| **3. Зачем нужны слоты**         | Слот — атом единицы данных, которую можно целиком **передвинуть** между узлами. Это упрощает шардирование: вместо перебалансировки миллионов ключей перекидываем пачку слотов. |
| **4. Распределение по мастерам** | В кластере с `N` мастеров каждый держит свой **под‑диапазон** слотов: `Master‑A:` 0‑5460 `Master‑B:` 5461‑10922 `Master‑C:` 10923‑16383 (пример для 3 узлов).                  |
| **5. Что делает клиент**         | Библиотека хранит карту «слот → мастер». При запросе определяет слот, сразу стучится в нужный узел. Если карта устарела, сервер шлёт редирект `MOVED`, и клиент обновляется.   |
| **6. Ресайз кластера**           | Добавили новый узел → админ утилитой `redis‑cli --cluster rebalance` отдаёт ему часть слотов (команда сама мигрирует ключи). Происходит **без даунтайма**.                     |
| **7. Ограничение**               | Максимум **16 384** мастер‑«шардов» (по одному слоту — теоретически). На практике узлов всегда намного меньше; слоты — просто удобная «мелкая дробь».                          |

> Кратко: 16384 слота — это фиксированная «шкала» для равномерного и управляемого распределения ключей по мастерам, позволяющая динамично добавлять/удалять узлы и мигрировать данные без сложной переиндексации каждого ключа.

---

## 6. Инструменты (буквально в два слова, что делают)

* **Redis Sentinel** — следит за мастером, автоматический failover, выдаёт новый адрес клиентам.  
* **redis‑cli `--cluster`** — утилита для bootstrap / reshard Cluster.  
* **RedisInsight** — графическое «посмотреть ключи, графики CPU/RAM».  
* **RedisGears / Functions** — сервер‑сайд скрипты на Python/Lua (фильтры, ETL).  
* **redis_exporter + Prometheus + Grafana** — метрики и алерты.

---

## 7. Гарантии (на пальцах)

| Слой | Что гарантирует | Кому доверять нельзя |
|------|-----------------|-----------------------|
| **Сетевой протокол** | TCP ⇒ порядок, без потерь внутри соединения | Обрыв соединения → клиент должен ретраить |
| **Atomicity одной команды** | Команда (`SET`, `HSET` …) выполняется целиком или не выполняется | Мульти‑узловые пайплайны в Cluster |
| **MULTI/EXEC** | Транзакция атомична **на одном узле** | На разных шардах нет атомарности |
| **Durability** | Зависит от AOF/RDB настроек и fsync | RAM‑только инстансы |
| **Consistency между репликами** | Eventually (асинхронная репликация) | Потеря последних N записей при failover |

---

## 8. Сценарии использования (быстрый справочник)

| Сценарий | Структура | TTL/Политика | Пример команды/паттерн |
|----------|-----------|--------------|------------------------|
| **API‑кеш** | String | `EX 60`, LRU | `SET user:42:profile … EX 60` |
| **Session store** | Hash / String | TTL = время сессии | `HSET sess:abc uid 42` |
| **Task queue** | List / Stream | Без TTL | `LPUSH q:mail id‑123` |
| **Leader‑board** | Sorted Set | Без TTL | `ZINCRBY lb:game 10 user42` |
| **Unique visitors** | HyperLogLog | Сбрасывать по суткам | `PFADD hll:20250421 ip` |
| **Pub/Sub** | Канал‑строка | - | `PUBLISH events "order:paid"` |

---

## 9. Производительность — от чего зависит и как ускорять

* **Собери метрики**: `INFO stats`, `slowlog get`.  
* **Пайплайны** — 20‑X ускорение на батч‑командах.  
* **Выбор структур** → `ZPOPMIN` вместо сортировки вручную.  
* **Оптимизируй память**: короткие ключи, pack‑encoding в Hash/Set при малом размере.  
* **Снимай снапшоты вне пиков** — иначе fork заблокирует мир на секунды.

---

## 10. Антипаттерны (кратко напоминалка)

1. Хранить гигабайтные файлы —> S3!  
2. `appendfsync always` на нагруженном API — в production не вывозит диск.  
3. `maxmemory-policy noeviction` без мониторинга — приложение падает по OOM.  
4. Один Redis без реплик → всё ломается при любой перезагрузке.  
5. Нулевые `timeout` у клиента — зависшие коннекты съедят `client‑max-input-buffer`.  

---

## 11. Репликация и шардирование — что ускоряют и как выбрать

| Приём | Что **ускоряет / улучшает** | Когда **точно** нужен | «Грабли» | Быстрый чек‑лист выбора |
|-------|----------------------------|----------------------|----------|------------------------|
| **Репликация**<br>(Master → Replica) | • Снимает **часть чтений** с мастера.<br>• Делает кластер **устойчивым к падению одного узла** (RTO < 5 с). | • Чтений > 70 % CPU‑мастера.<br>• Требуется SLA > 99.9 %.<br>• Нужно убрать бэкапы / аналитические сканы с мастера. | • Асинхронная → последние записи могут потеряться.<br>• Память и I/O растут × кол‑ву реплик. | 1. Возьми **1 реплику/мастер** для HA.<br>2. Добавь вторую, если чтения всё ещё упираются в CPU.<br>3. Разноси мастер и реплики по разным AZ. |
| **Шардирование**<br>(Redis Cluster) | • Делит **данные и запросы на несколько ядер/узлов** → масштабирует write‑TPS и RAM. | • Данные > 70 % RAM узла.<br>• Write/QPS упираются в 1 CPU.<br>• Нужен горизонтальный рост без даунтайма. | • Мульти‑ключевые операции «по‑всем» ключам дороже (cross‑slot).<br>• Клиенты _должны_ понимать Cluster. | 1. Посчитай объём (`D_total / (RAM_узла × 0.65)` → мастера).<br>2. Минимум **3 мастера**, иначе нет кворума.<br>3. На каждый мастер ≥ 1 реплика. |
| **Sentinel vs Cluster** | Sentinel — только failover *одного* мастера с репликами.<br>Cluster — failover + шардирование (16384 слота). | Sentinel: «один узел, но с HA».<br>Cluster: «несколько узлов, нужен рост». | Sentinel не уменьшает объём данных; Cluster требует Cluster‑клиент. |  `< 3 узлов` → Sentinel.<br> `≥ 3 мастеров или нужен рост` → Cluster. |

> **Запомнить**:  
> – Репликация = чтения + высокая доступность.  
> – Шардирование = память + записи + масштаб.  
> Оба механизма обычно включают **одновременно**: «N мастеров для объёма, по 1‑2 реплике для HA и чтений».
